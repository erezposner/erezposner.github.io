<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>root</title>
	<atom:link href="./index.html" rel="self" type="application/rss+xml" />
	<link>./../../../index.html</link>
	<description></description>
	<lastBuildDate>Wed, 23 Aug 2023 12:27:29 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.3</generator>

<image>
	<url>./../../../wp-content/uploads/2021/01/tr.png</url>
	<title>root</title>
	<link>./../../../index.html</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Colonoscopy Landmark Detection using Vision Transformers</title>
		<link>./../../../2023/08/23/colonoscopy-landmark-detection-using-vision-transformers/index.html</link>
					<comments>./../../../2023/08/23/colonoscopy-landmark-detection-using-vision-transformers/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Wed, 23 Aug 2023 12:19:15 +0000</pubDate>
				<category><![CDATA[MyPublications]]></category>
		<guid isPermaLink="false">./../../../index.html?p=848</guid>

					<description><![CDATA[Colonoscopy is a routine outpatient procedure used to examine the colon and rectum for any abnormalities including polyps, diverticula and narrowing of colon structures. A significant amount of the clinician&#8217;s time is spent in post-processing snapshots taken during the colonoscopy procedure, for maintaining medical records or further investigation. Automating this step can save time and&#8230;&#160;<a href="./../../../2023/08/23/colonoscopy-landmark-detection-using-vision-transformers/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">Colonoscopy Landmark Detection using Vision Transformers</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Colonoscopy is a routine outpatient procedure used to examine the colon and rectum for any abnormalities including polyps, diverticula and narrowing of colon structures. A significant amount of the clinician&#8217;s time is spent in post-processing snapshots taken during the colonoscopy procedure, for maintaining medical records or further investigation. Automating this step can save time and improve the efficiency of the process. In our work, we have collected a dataset of 120 colonoscopy videos and 2416 snapshots taken during the procedure, that have been annotated by experts. Further, we have developed a novel, vision-transformer based landmark detection algorithm that identifies key anatomical landmarks (the appendiceal orifice, ileocecal valve/cecum landmark and rectum retroflexion) from snapshots taken during colonoscopy. Our algorithm uses an adaptive gamma correction during preprocessing to maintain a consistent brightness for all images. We then use a vision transformer as the feature extraction backbone and a fully connected network based classifier head to categorize a given frame into four classes: the three landmarks or a non-landmark frame. We compare the vision transformer (ViT-B/16) backbone with ResNet-101 and ConvNext-B backbones that have been trained similarly. We report an accuracy of 82% with the vision transformer backbone on a test dataset of snapshots.</p>



<p><a href="https://arxiv.org/abs/2209.11304">The Full paper is available on Arxiv</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2023/08/23/colonoscopy-landmark-detection-using-vision-transformers/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>SimCol3D &#8211; 3D Reconstruction during Colonoscopy Challenge</title>
		<link>./../../../2023/07/23/simcol3d-3d-reconstruction-during-colonoscopy-challenge/index.html</link>
					<comments>./../../../2023/07/23/simcol3d-3d-reconstruction-during-colonoscopy-challenge/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Sun, 23 Jul 2023 12:25:00 +0000</pubDate>
				<category><![CDATA[MyPublications]]></category>
		<guid isPermaLink="false">./../../../index.html?p=854</guid>

					<description><![CDATA[Colorectal cancer is one of the most common cancers in the world. While colonoscopy is an effective screening technique, navigating an endoscope through the colon to detect polyps is challenging. A 3D map of the observed surfaces could enhance the identification of unscreened colon tissue and serve as a training platform. However, reconstructing the colon&#8230;&#160;<a href="./../../../2023/07/23/simcol3d-3d-reconstruction-during-colonoscopy-challenge/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">SimCol3D &#8211; 3D Reconstruction during Colonoscopy Challenge</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Colorectal cancer is one of the most common cancers in the world. While colonoscopy is an effective screening technique, navigating an endoscope through the colon to detect polyps is challenging. A 3D map of the observed surfaces could enhance the identification of unscreened colon tissue and serve as a training platform. However, reconstructing the colon from video footage remains unsolved due to numerous factors such as self-occlusion, reflective surfaces, lack of texture, and tissue deformation that limit feature-based methods. Learning-based approaches hold promise as robust alternatives, but necessitate extensive datasets. By establishing a benchmark, the 2022 EndoVis sub-challenge SimCol3D aimed to facilitate data-driven depth and pose prediction during colonoscopy. The challenge was hosted as part of MICCAI 2022 in Singapore. Six teams from around the world and representatives from academia and industry participated in the three sub-challenges: synthetic depth prediction, synthetic pose prediction, and real pose prediction. This paper describes the challenge, the submitted methods, and their results. We show that depth prediction in virtual colonoscopy is robustly solvable, while pose estimation remains an open research question.</p>



<p><a href="https://www.researchgate.net/publication/372547675_SimCol3D_--_3D_Reconstruction_during_Colonoscopy_Challenge#fullTextFileContent">The Full paper is available here</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2023/07/23/simcol3d-3d-reconstruction-during-colonoscopy-challenge/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>ColNav: Real-Time Colon Navigation for Colonoscopy</title>
		<link>./../../../2023/06/07/colnav-real-time-colon-navigation-for-colonoscopy/index.html</link>
					<comments>./../../../2023/06/07/colnav-real-time-colon-navigation-for-colonoscopy/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Wed, 07 Jun 2023 12:22:00 +0000</pubDate>
				<category><![CDATA[MyPublications]]></category>
		<guid isPermaLink="false">./../../../index.html?p=852</guid>

					<description><![CDATA[Colorectal cancer screening through colonoscopy continues to be the dominant global standard, as it allows identifying pre-cancerous or adenomatous lesions and provides the ability to remove them during the procedure itself. Nevertheless, failure by the endoscopist to identify such lesions increases the likelihood of lesion progression to subsequent colorectal cancer. Ultimately, colonoscopy remains operator-dependent, and&#8230;&#160;<a href="./../../../2023/06/07/colnav-real-time-colon-navigation-for-colonoscopy/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">ColNav: Real-Time Colon Navigation for Colonoscopy</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Colorectal cancer screening through colonoscopy continues to be the dominant global standard, as it allows identifying pre-cancerous or adenomatous lesions and provides the ability to remove them during the procedure itself. Nevertheless, failure by the endoscopist to identify such lesions increases the likelihood of lesion progression to subsequent colorectal cancer. Ultimately, colonoscopy remains operator-dependent, and the wide range of quality in colonoscopy examinations among endoscopists is influenced by variations in their technique, training, and diligence. This paper presents a novel real-time navigation guidance system for Optical Colonoscopy (OC). Our proposed system employs a real-time approach that displays both an unfolded representation of the colon and a local indicator directing to un-inspected areas. These visualizations are presented to the physician during the procedure, providing actionable and comprehensible guidance to un-surveyed areas in real-time, while seamlessly integrating into the physician&#8217;s workflow. Through coverage experimental evaluation, we demonstrated that our system resulted in a higher polyp recall (PR) and high inter-rater reliability with physicians for coverage prediction. These results suggest that our real-time navigation guidance system has the potential to improve the quality and effectiveness of Optical Colonoscopy and ultimately benefit patient outcomes.</p>



<p><a href="https://arxiv.org/abs/2306.04269#:~:text=This%20paper%20presents%20a%20novel,directing%20to%20un%2Dinspected%20areas.">The Full paper is available on Arxiv</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2023/06/07/colnav-real-time-colon-navigation-for-colonoscopy/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Estimating the coverage in 3d reconstructions of the colon from colonoscopy videos</title>
		<link>./../../../2022/10/19/estimating-the-coverage-in-3d-reconstructions-of-the-colon-from-colonoscopy-videos/index.html</link>
					<comments>./../../../2022/10/19/estimating-the-coverage-in-3d-reconstructions-of-the-colon-from-colonoscopy-videos/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Wed, 19 Oct 2022 12:19:00 +0000</pubDate>
				<category><![CDATA[MyPublications]]></category>
		<guid isPermaLink="false">./../../../index.html?p=850</guid>

					<description><![CDATA[Published &#8211; Imaging Systems for GI Endoscopy workshop, the 25th International Conference on Medical Image Computing and Computer Assisted Intervention &#8211; MICCAI 2022 ISGIE Colonoscopy is the most common procedure for early detection and removal of polyps, a critical component of colorectal cancer prevention. Insufficient visual coverage of the colon surface during the procedure often&#8230;&#160;<a href="./../../../2022/10/19/estimating-the-coverage-in-3d-reconstructions-of-the-colon-from-colonoscopy-videos/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">Estimating the coverage in 3d reconstructions of the colon from colonoscopy videos</span></a>]]></description>
										<content:encoded><![CDATA[
<p><strong>Published</strong> &#8211; Imaging Systems for GI Endoscopy workshop, the 25th International Conference on Medical Image Computing and Computer Assisted Intervention &#8211; MICCAI 2022 ISGIE</p>



<p>Colonoscopy is the most common procedure for early detection and removal of polyps, a critical component of colorectal cancer prevention. Insufficient visual coverage of the colon surface during the procedure often results in missed polyps. To mitigate this issue, reconstructing the 3D surfaces of the colon in order to visualize the missing regions has been proposed. However, robustly estimating the local and global coverage from such a reconstruction has not been thoroughly investigated until now. In this work, we present a new method to estimate the coverage from a reconstructed colon pointcloud. Our method splits a reconstructed colon into segments and estimates the coverage of each segment by estimating the area of the missing surfaces. We achieve a mean absolute coverage error of 3-6\% on colon segments generated from synthetic colonoscopy data and real colonography CT scans. In addition, we show good qualitative results on colon segments reconstructed from real colonoscopy videos.</p>



<p><a href="https://arxiv.org/abs/2210.10459">The Full paper is available on Arxiv</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2022/10/19/estimating-the-coverage-in-3d-reconstructions-of-the-colon-from-colonoscopy-videos/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>C3Fusion: Consistent Contrastive Colon Fusion, Towards Deep SLAM in Colonoscopy</title>
		<link>./../../../2022/06/01/c3fusion-consistent-contrastive-colon-fusion-towards-deep-slam-in-colonoscopy/index.html</link>
					<comments>./../../../2022/06/01/c3fusion-consistent-contrastive-colon-fusion-towards-deep-slam-in-colonoscopy/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Wed, 01 Jun 2022 12:17:00 +0000</pubDate>
				<category><![CDATA[MyPublications]]></category>
		<guid isPermaLink="false">./../../../index.html?p=846</guid>

					<description><![CDATA[3D colon reconstruction from Optical Colonoscopy (OC) to detect non-examined surfaces remains an unsolved problem. The challenges arise from the nature of optical colonoscopy data, characterized by highly reflective low-texture surfaces, drastic illumination changes and frequent tracking loss. Recent methods demonstrate compelling results, but suffer from: (1) frangible frame-to-frame (or frame-to-model) pose estimation resulting in&#8230;&#160;<a href="./../../../2022/06/01/c3fusion-consistent-contrastive-colon-fusion-towards-deep-slam-in-colonoscopy/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">C3Fusion: Consistent Contrastive Colon Fusion, Towards Deep SLAM in Colonoscopy</span></a>]]></description>
										<content:encoded><![CDATA[
<p>3D colon reconstruction from Optical Colonoscopy (OC) to detect non-examined surfaces remains an unsolved problem. The challenges arise from the nature of optical colonoscopy data, characterized by highly reflective low-texture surfaces, drastic illumination changes and frequent tracking loss. Recent methods demonstrate compelling results, but suffer from: (1) frangible frame-to-frame (or frame-to-model) pose estimation resulting in many tracking failures; or (2) rely on point-based representations at the cost of scan quality. In this paper, we propose a novel reconstruction framework that addresses these issues end to end, which result in both quantitatively and qualitatively accurate and robust 3D colon reconstruction. Our SLAM approach, which employs correspondences based on contrastive deep features, and deep consistent depth maps, estimates globally optimized poses, is able to recover from frequent tracking failures, and estimates a global consistent 3D model; all within a single framework. We perform an extensive experimental evaluation on multiple synthetic and real colonoscopy videos, showing high-quality results and comparisons against relevant baselines.</p>



<p><a href="https://arxiv.org/abs/2206.01961">The Full paper is available on Arxiv</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2022/06/01/c3fusion-consistent-contrastive-colon-fusion-towards-deep-slam-in-colonoscopy/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>End-to-End 3D Point Cloud Learning for Registration Task Using Virtual Correspondences</title>
		<link>./../../../2021/01/03/end-to-end-3d-point-cloud-learning-for-registration-task-usingvirtual-correspondences/index.html</link>
					<comments>./../../../2021/01/03/end-to-end-3d-point-cloud-learning-for-registration-task-usingvirtual-correspondences/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Sun, 03 Jan 2021 21:18:22 +0000</pubDate>
				<category><![CDATA[Papers 2020]]></category>
		<guid isPermaLink="false">./../../../index.html?p=815</guid>

					<description><![CDATA[tl;dr End-to-end deep-learning based approach to resolve the point cloud registration problem. Main steps are: LPD-Net is used to extract features and aggregate them with graph network. Self &#8211; attention mechanism is utilized to enhance the structure information of the PCL Cross &#8211; attention mechanism is deisned to enhace correpondences between the two PCLs Registration&#8230;&#160;<a href="./../../../2021/01/03/end-to-end-3d-point-cloud-learning-for-registration-task-usingvirtual-correspondences/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">End-to-End 3D Point Cloud Learning for Registration Task Using Virtual Correspondences</span></a>]]></description>
										<content:encoded><![CDATA[
<h2 class="wp-block-heading">tl;dr</h2>



<p>End-to-end deep-learning based approach to resolve the point cloud registration problem. Main steps are:</p>



<ul><li>LPD-Net is used to extract features and aggregate them with graph network. </li><li>Self &#8211; attention mechanism is utilized to enhance the structure information of the PCL</li><li>Cross &#8211; attention mechanism is deisned to enhace correpondences between the two PCLs</li><li>Registration is then solved using SVD</li></ul>



<h2 class="wp-block-heading">Overall Impression</h2>



<p>The authrs claim to support registration without initial predicition. They present LPD-Registration. </p>



<p>LPD-Registration utilizes a modified version of LPD-Net to extract the local features and aggregates them with the k-NN based graph network Nx3 -&gt; N512 as it&#8217;s output. </p>



<p>They remove the process of aggregating local features to a global descriptor vector, and simplify the network by only performing graph-based neighborhood aggregation in the Cartesian space as well as the feature space.</p>



<p>The &#8220;Self-supervised pre-training algorithm for the revised LPD-Net&#8221; is quite shady and requires 1-to-1 correspondence between P1 and P2.</p>



<p>self-attention mechanism is used to enhance the static structure information in the largescale point cloud and exclude the parts which are difficult to match.  basically uses softmax on the descriptors as a damper to points weights.</p>



<p>Cross Attention is used to enhance correpondences and a virtual correponding points is presented to offer alternative correspondences out of the Top-K points.</p>



<p>The loss for the training is correspondence loss including the accuracy of the virtual points as well. Worth mentioning that pose loss is not used here.</p>



<h2 class="wp-block-heading">Key Ideas</h2>



<ul><li> investigate the point cloud registration problem of large-scale point<br>clouds with local sparsity and partially correspondence</li></ul>



<ul><li>end-to-end deep-learning approach is proposed by incorporating the self attention and cross attention mechanism to enhance the static structure information and indicate the correspondence of two point clouds.</li><li></li></ul>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2021/01/03/end-to-end-3d-point-cloud-learning-for-registration-task-usingvirtual-correspondences/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>A single camera based floating virtual keyboard with improved touch detection</title>
		<link>./../../../2020/12/31/a-single-camera-based-floating-virtual-keyboard-with-improved-touch-detection/index.html</link>
					<comments>./../../../2020/12/31/a-single-camera-based-floating-virtual-keyboard-with-improved-touch-detection/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Thu, 31 Dec 2020 22:07:59 +0000</pubDate>
				<category><![CDATA[MyPublications]]></category>
		<guid isPermaLink="false">./../../../index.html?p=481</guid>

					<description><![CDATA[Published in:&#160;2012 IEEE 27th Convention of Electrical and Electronics Engineers in Israel Virtual keyboard enables user typing on any surface, including a plain paper or your desk. Some virtual keyboards give vibration feedback; some are projected on the typing surface, while others give different kind of visual feedback such as showing it on a smart&#8230;&#160;<a href="./../../../2020/12/31/a-single-camera-based-floating-virtual-keyboard-with-improved-touch-detection/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">A single camera based floating virtual keyboard with improved touch detection</span></a>]]></description>
										<content:encoded><![CDATA[
<h4 class="wp-block-heading"><strong>Published in:&nbsp;</strong><a href="https://ieeexplore.ieee.org/xpl/conhome/6365376/proceeding">2012 IEEE 27th Convention of Electrical and Electronics Engineers in Israel</a><br></h4>



<p>Virtual keyboard enables user typing on any surface, including a plain paper or your desk. Some virtual keyboards give vibration feedback; some are projected on the typing surface, while others give different kind of visual feedback such as showing it on a smart phone&#8217;s screen. The user “presses” the virtual keys thus typing the desired input text. In this work we have implemented a single standard camera-based virtual keyboard, by improving shadow-based touch detection. The proposed solution is applicable to any surface. The system has been implemented on an Android phone, operates in real time, and gives excellent results.</p>



<p><a href="https://ieeexplore.ieee.org/document/6377072">The full paper is available on IEEE</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2020/12/31/a-single-camera-based-floating-virtual-keyboard-with-improved-touch-detection/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Automatic identification of positive or negative language</title>
		<link>./../../../2020/12/31/automatic-identification-of-positive-or-negative-language/index.html</link>
					<comments>./../../../2020/12/31/automatic-identification-of-positive-or-negative-language/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Thu, 31 Dec 2020 22:06:33 +0000</pubDate>
				<category><![CDATA[MyPublications]]></category>
		<guid isPermaLink="false">./../../../index.html?p=478</guid>

					<description><![CDATA[Published in:&#160;2012 IEEE 27th Convention of Electrical and Electronics Engineers in Israel Personal coaching, performed by professionals such as psychologists, usually includes training for business as well as social situations such as job interviews, business meetings, interaction with a customer service provider, and more. This requires careful preparation in which, among other traits, the trainees&#8230;&#160;<a href="./../../../2020/12/31/automatic-identification-of-positive-or-negative-language/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">Automatic identification of positive or negative language</span></a>]]></description>
										<content:encoded><![CDATA[
<h4 class="wp-block-heading"><strong>Published in:&nbsp;</strong><a href="https://ieeexplore.ieee.org/xpl/conhome/6365376/proceeding">2012 IEEE 27th Convention of Electrical and Electronics Engineers in Israel</a></h4>



<p>Personal coaching, performed by professionals such as psychologists, usually includes training for business as well as social situations such as job interviews, business meetings, interaction with a customer service provider, and more. This requires careful preparation in which, among other traits, the trainees need to pay attention to the words they choose in the interaction, in order to make a positive impression. To achieve this goal, we have developed a coaching system using speech recognition, which enables both monitoring by the coaching professional and self-training by the user. By providing timely indications as to when the user employs positive or negative expressions as defined by the psychologist, the system helps users develop self-control and awareness regarding the language they use. The system consists of adjusted voice activity detection (VAD) and key word spotting (KWS) algorithms, implemented together with an interactive UI into an Android-based application, available on cellular phones.</p>



<p><a href="https://ieeexplore.ieee.org/abstract/document/6377047">The full paper is available on IEEE</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2020/12/31/automatic-identification-of-positive-or-negative-language/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Motion Segmentation Using Locally Affine Atom Voting</title>
		<link>./../../../2020/12/31/motion-segmentation-using-locally-affine-atom-voting/index.html</link>
					<comments>./../../../2020/12/31/motion-segmentation-using-locally-affine-atom-voting/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Thu, 31 Dec 2020 22:03:44 +0000</pubDate>
				<category><![CDATA[MyPublications]]></category>
		<guid isPermaLink="false">./../../../index.html?p=473</guid>

					<description><![CDATA[We present a novel method for motion segmentation called LAAV (Locally Affine Atom Voting). Our model&#8217;s main novelty is using sets of features to segment motion for all features in the scene. LAAV acts as a pre-processing pipeline stage for features in the image, followed by a fine-tuned version of the state-of-the-art Random Voting (RV)&#8230;&#160;<a href="./../../../2020/12/31/motion-segmentation-using-locally-affine-atom-voting/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">Motion Segmentation Using Locally Affine Atom Voting</span></a>]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-image is-style-default"><figure class="aligncenter size-large is-resized"><img decoding="async" fetchpriority="high" src="./../../../wp-content/uploads/2020/12/111.png" alt="" class="wp-image-475" width="366" height="315"/></figure></div>



<p>We present a novel method for motion segmentation called LAAV (Locally Affine Atom Voting). Our model&#8217;s main novelty is using sets of features to segment motion for all features in the scene. LAAV acts as a pre-processing pipeline stage for features in the image, followed by a fine-tuned version of the state-of-the-art Random Voting (RV) method. Unlike standard approaches, LAAV segments motion using feature-set affinities instead of pair-wise affinities between all features; therefore, it significantly simplifies complex scenarios and reduces the computational cost without a loss of accuracy. We describe how the challenges encountered by using previously suggested approaches are addressed using our model. We then compare our algorithm with several state-of-the-art methods. Experiments shows that our approach achieves the most accurate motion segmentation results and, in the presence of measurement noise, achieves comparable results to the other algorithms.</p>



<p><a href="https://arxiv.org/abs/1907.06091">The full paper is available on arxiv</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2020/12/31/motion-segmentation-using-locally-affine-atom-voting/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>How To Create Awesome Noise That Is Actually Real</title>
		<link>./../../../2020/12/31/how-to-create-awesome-noise-that-is-actually-real/index.html</link>
					<comments>./../../../2020/12/31/how-to-create-awesome-noise-that-is-actually-real/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Thu, 31 Dec 2020 21:58:55 +0000</pubDate>
				<category><![CDATA[Blogs]]></category>
		<guid isPermaLink="false">./../../../index.html?p=469</guid>

					<description><![CDATA[if you’ve ever looked at a digital image, you couldn’t fail to notice this grainy agitated noise differentiating two adjacent pixels viewing on practically the same object. Why on God’s green earth would two pixels capturing clear sky, for example, won’t have the same gray level value? The full story is available on Medium]]></description>
										<content:encoded><![CDATA[
<p>if you’ve ever looked at a digital image, you couldn’t fail to notice this grainy agitated noise differentiating two adjacent pixels viewing on practically the same object. Why on God’s green earth would two pixels capturing clear sky, for example, won’t have the same gray level value?</p>



<p><a href="https://arxiv.org/abs/1907.06091">The full story is available on Medium</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2020/12/31/how-to-create-awesome-noise-that-is-actually-real/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
	</channel>
</rss>

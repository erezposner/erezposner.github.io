<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Blogs</title>
	<atom:link href="./index.html" rel="self" type="application/rss+xml" />
	<link>./../../../index.html</link>
	<description></description>
	<lastBuildDate>Thu, 31 Dec 2020 22:01:58 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.6</generator>

<image>
	<url>./../../../wp-content/uploads/2021/01/tr.png</url>
	<title>Blogs</title>
	<link>./../../../index.html</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>How To Create Awesome Noise That Is Actually Real</title>
		<link>./../../../2020/12/31/how-to-create-awesome-noise-that-is-actually-real/index.html</link>
					<comments>./../../../2020/12/31/how-to-create-awesome-noise-that-is-actually-real/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Thu, 31 Dec 2020 21:58:55 +0000</pubDate>
				<category><![CDATA[Blogs]]></category>
		<guid isPermaLink="false">./../../../index.html?p=469</guid>

					<description><![CDATA[if you’ve ever looked at a digital image, you couldn’t fail to notice this grainy agitated noise differentiating two adjacent pixels viewing on practically the same object. Why on God’s green earth would two pixels capturing clear sky, for example, won’t have the same gray level value? The full story is available on Medium]]></description>
										<content:encoded><![CDATA[
<p>if you’ve ever looked at a digital image, you couldn’t fail to notice this grainy agitated noise differentiating two adjacent pixels viewing on practically the same object. Why on God’s green earth would two pixels capturing clear sky, for example, won’t have the same gray level value?</p>



<p><a href="https://arxiv.org/abs/1907.06091">The full story is available on Medium</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2020/12/31/how-to-create-awesome-noise-that-is-actually-real/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Speedup your CNN using Fast Dense Feature Extraction and PyTorch</title>
		<link>./../../../2020/12/28/speedup-your-cnn-using-fast-dense-feature-extraction-and-pytorch/index.html</link>
					<comments>./../../../2020/12/28/speedup-your-cnn-using-fast-dense-feature-extraction-and-pytorch/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Mon, 28 Dec 2020 20:33:42 +0000</pubDate>
				<category><![CDATA[Blogs]]></category>
		<guid isPermaLink="false">https://erezposnerportfolio.wordpress.com/?p=106</guid>

					<description><![CDATA[Back in March, we open-sourced our&#160;implementation of “Fast Dense Feature Extraction with CNN&#8217;s that have Pooling or Striding Layers”, Although not broadly known, The 2017 BMVC published paper offers an efficient and elegant solution on how to avoid computational redundancy when using patch based Convolution Neural networks. So in this post I’ll explain how the&#8230;&#160;<a href="./../../../2020/12/28/speedup-your-cnn-using-fast-dense-feature-extraction-and-pytorch/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">Speedup your CNN using Fast Dense Feature Extraction and PyTorch</span></a>]]></description>
										<content:encoded><![CDATA[
<p id="d17e">Back in March, we open-sourced our&nbsp;<a href="https://github.com/erezposner/Fast_Dense_Feature_Extraction">implementation of “Fast Dense Feature Extraction with CNN&#8217;s that have Pooling or Striding Layers</a>”, Although not broadly known, The 2017 BMVC published paper offers an efficient and elegant solution on how to avoid computational redundancy when using patch based Convolution Neural networks. So in this post I’ll explain how the model works and show how to use it in a real applications.</p>



<p id="37c4">I’ll cover two things: First, an&nbsp;overview of the method named “Fast Dense Feature Extraction with CNN&#8217;s that have Pooling or Striding Layers”. And, second, how to use this approach on an existing trained patch network to speed up inference time.</p>



<p><a href="https://towardsdatascience.com/speedup-your-cnn-using-fast-dense-feature-extraction-and-pytorch-dc32acbf12ef">The full story is available Medium</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2020/12/28/speedup-your-cnn-using-fast-dense-feature-extraction-and-pytorch/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>How To Perform Image Restoration Absolutely DataSet Free</title>
		<link>./../../../2020/12/28/how-to-perform-image-restoration-absolutely-dataset-free/index.html</link>
					<comments>./../../../2020/12/28/how-to-perform-image-restoration-absolutely-dataset-free/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Mon, 28 Dec 2020 20:32:34 +0000</pubDate>
				<category><![CDATA[Blogs]]></category>
		<guid isPermaLink="false">https://erezposnerportfolio.wordpress.com/?p=103</guid>

					<description><![CDATA[Image restoration with neural networks without learning Deep learning requires a large amount of data.&#160;This phrase has become popular among people who consider applying deep learning methods to their data. Concerns often made when not having “big” enough data mainly derive from the common belief that deep learning&#160;only&#160;works using massive amount of data. Well, This&#8230;&#160;<a href="./../../../2020/12/28/how-to-perform-image-restoration-absolutely-dataset-free/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">How To Perform Image Restoration Absolutely DataSet Free</span></a>]]></description>
										<content:encoded><![CDATA[
<p><strong>Image restoration with neural networks without learning</strong></p>



<p id="f85d"><strong>Deep learning requires a large amount of data.&nbsp;</strong>This phrase has become popular among people who consider applying deep learning methods to their data. Concerns often made when not having “big” enough data mainly derive from the common belief that deep learning&nbsp;<mark>only</mark>&nbsp;works using massive amount of data. Well, This is not true.</p>



<p id="f633">Although for some cases you actually do need massive amount of data, there are some networks that could be trained on a single image. On top of that, In practice, even without large datasets, the structure of the network itself could be preventing deep networks from over-fitting.</p>



<p><a href="https://towardsdatascience.com/how-to-perform-image-restoration-absolutely-dataset-free-d08da1a1e96d">The full story is available Medium</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2020/12/28/how-to-perform-image-restoration-absolutely-dataset-free/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Detection Free Human Instance Segmentation using Pose2Seg and PyTorch</title>
		<link>./../../../2020/12/28/detection-free-human-instance-segmentation-using-pose2seg-and-pytorch/index.html</link>
					<comments>./../../../2020/12/28/detection-free-human-instance-segmentation-using-pose2seg-and-pytorch/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Mon, 28 Dec 2020 20:31:22 +0000</pubDate>
				<category><![CDATA[Blogs]]></category>
		<guid isPermaLink="false">https://erezposnerportfolio.wordpress.com/?p=100</guid>

					<description><![CDATA[Taking into consideration the uniqueness of Human In recent years, research related to “humans” in the computer vision community has become increasingly active because of the high demand for real-life applications, among them is instance segmentation. The standard approach to image instance segmentation is to perform the object detection first, and then segment the object&#8230;&#160;<a href="./../../../2020/12/28/detection-free-human-instance-segmentation-using-pose2seg-and-pytorch/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">Detection Free Human Instance Segmentation using Pose2Seg and PyTorch</span></a>]]></description>
										<content:encoded><![CDATA[
<p><strong>Taking into consideration the uniqueness of Human</strong></p>



<p id="66d4">In recent years, research related to “humans” in the computer vision community has become increasingly active because of the high demand for real-life applications, among them is instance segmentation.</p>



<p id="b5c7">The standard approach to image instance segmentation is to perform the object detection first, and then segment the object from the detection bounding-box. More recently, deep learning methods like&nbsp;<a href="https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46">Mask R-CNN</a>&nbsp;perform them jointly. However, as human associated tasks becoming more common like human recognition, tracking etc. one might wonder why does the uniqueness of the “human” category does not taken into account.</p>



<p><a href="https://towardsdatascience.com/detection-free-human-instance-segmentation-using-pose2seg-and-pytorch-72f48dc4d23e">The full story is available Medium</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2020/12/28/detection-free-human-instance-segmentation-using-pose2seg-and-pytorch/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Deep Video Portraits</title>
		<link>./../../../2020/12/28/whats-on-my-desk/index.html</link>
					<comments>./../../../2020/12/28/whats-on-my-desk/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Mon, 28 Dec 2020 17:49:27 +0000</pubDate>
				<category><![CDATA[Blogs]]></category>
		<guid isPermaLink="false">https://erezposnerportfolio.wordpress.com/?p=3</guid>

					<description><![CDATA[Photo-realistic re-animation of portrait videos using only an input video Synthesizing and editing video portraits—i.e., videos framed to show a person’s head and upper body—is an important problem in computer graphics, with applications in video editing and movie postproduction, visual effects, visual dubbing, virtual reality, and telepresence, among others. The problem of synthesizing a photo-realistic&#8230;&#160;<a href="./../../../2020/12/28/whats-on-my-desk/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">Deep Video Portraits</span></a>]]></description>
										<content:encoded><![CDATA[
<p><strong>Photo-realistic re-animation of portrait videos using only an input video</strong></p>



<p id="470c">Synthesizing and editing video portraits—i.e., videos framed to show a person’s head and upper body—is an important problem in computer graphics, with applications in video editing and movie postproduction, visual effects, visual dubbing, virtual reality, and telepresence, among others.</p>



<p id="c77b">The problem of synthesizing a photo-realistic video portrait of a target actor that mimics the actions of a source actor—and especially where the source and target actors can be different subjects—is still an open problem.</p>



<p id="1921">There hasn’t been an approach that enables one to take full control of the rigid head pose, face expressions, and eye motion of the target actor; even face identity can be modified to some extent. Until now.</p>



<p id="b3dc">In this post, I’m going to review&nbsp;<a href="https://arxiv.org/pdf/1805.11714.pdf">“Deep Video Portraits”</a>, which presents a novel approach that enables photo-realistic re-animation of portrait videos using only an input video.</p>



<p id="c35c">In this post, I’ll cover two things: First, a short definition of a DeepFake. Second, an overview of the paper “Deep Video Portraits” in the words of the authors.</p>



<p><a href="https://heartbeat.fritz.ai/deep-video-portraits-f0f4a136546a">The full story is available Medium</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2020/12/28/whats-on-my-desk/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>HAMR — 3D Hand Shape and Pose Estimation from a Single RGB Image</title>
		<link>./../../../2020/12/28/my-daily-planner/index.html</link>
					<comments>./../../../2020/12/28/my-daily-planner/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Mon, 28 Dec 2020 17:49:26 +0000</pubDate>
				<category><![CDATA[Blogs]]></category>
		<guid isPermaLink="false">https://erezposnerportfolio.wordpress.com/?p=4</guid>

					<description><![CDATA[End-to-end Hand Mesh Recovery from a Monocular RGB Image. In recent years, research related to vision-based 3D image processing has become increasingly active, given its many applications in virtual reality (VR) and augmented reality (AR). Despite years of studies, however, there are still images that machines struggle to understand—one of those is images of human&#8230;&#160;<a href="./../../../2020/12/28/my-daily-planner/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">HAMR — 3D Hand Shape and Pose Estimation from a Single RGB Image</span></a>]]></description>
										<content:encoded><![CDATA[
<p><strong>End-to-end Hand Mesh Recovery from a Monocular RGB Image.</strong></p>



<p id="a16f">In recent years, research related to vision-based 3D image processing has become increasingly active, given its many applications in virtual reality (VR) and augmented reality (AR). Despite years of studies, however, there are still images that machines struggle to understand—one of those is images of human hands.</p>



<p id="8753">Hand image understanding targets the problem of recovering the spatial configuration of hands from natural RGB or/and depth images. This task has many applications, such as human-machine interaction and virtual/augmented reality, among others.</p>



<p id="8f35">Estimating the spatial configuration of hand images is very challenging due to the variations in appearance, self-occlusion, and complex articulations. While many existing works considered markerless image-based understanding, most of them require depth cameras or multi-view images to handle these difficulties.</p>



<p id="1493">Considering RGB cameras are more widely available than depth cameras, some recent work has started looking into 3D hand analysis from monocular RGB images, mainly focusing on estimating sparse 3D hand joint locations while ignoring dense 3D hand shapes.</p>



<p id="c184">However, many immersive VR and AR applications often require accurate estimation of both 3D hand pose and 3D hand shape. This brings about a more challenging task: How can we jointly estimate not only the 3D hand joint locations, but also the full 3D mesh of a hand’s surface from a single RGB image?</p>



<p><a href="https://heartbeat.fritz.ai/hamr-3d-hand-shape-and-pose-estimation-from-a-single-rgb-image-225cba26ce2">The full story is available Medium</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2020/12/28/my-daily-planner/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>ActiveStereoNet: The first deep learning solution for active stereo systems</title>
		<link>./../../../2020/12/28/travel-tips/index.html</link>
					<comments>./../../../2020/12/28/travel-tips/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Mon, 28 Dec 2020 17:49:25 +0000</pubDate>
				<category><![CDATA[Blogs]]></category>
		<guid isPermaLink="false">https://erezposnerportfolio.wordpress.com/?p=5</guid>

					<description><![CDATA[End-to-End Self-Supervised Learning for Active Stereo Systems Depth sensing is a classic problem with a long history of prior work. It’s at the heart of many tasks, from&#160;3D reconstruction&#160;to localization and tracking. Its applications span otherwise disparate research and product areas, including indoor mapping and architecture, autonomous cars, and human body and face tracking. With&#8230;&#160;<a href="./../../../2020/12/28/travel-tips/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">ActiveStereoNet: The first deep learning solution for active stereo systems</span></a>]]></description>
										<content:encoded><![CDATA[
<p><strong>End-to-End Self-Supervised Learning for Active Stereo Systems</strong></p>



<p id="136e">Depth sensing is a classic problem with a long history of prior work. It’s at the heart of many tasks, from&nbsp;<a href="https://heartbeat.fritz.ai/3d-face-reconstruction-with-position-map-regression-networks-36f0ac2d3ef1">3D reconstruction</a>&nbsp;to localization and tracking. Its applications span otherwise disparate research and product areas, including indoor mapping and architecture, autonomous cars, and human body and face tracking.</p>



<p id="8f9e">With interest in virtual and augmented reality rising, depth estimation has recently taken center stage. Depth sensors are revolutionizing computer vision by providing additional 3D information for many hard problems.</p>



<p id="f032">Although there are many types of depth sensor technologies (shown in Fig.1 below), they all have significant limitations.</p>



<ul><li><strong>Time of Flight&nbsp;</strong>(TOF) systems suffer from motion artifacts and multi-path interference.</li><li><strong>Structured light</strong>&nbsp;is vulnerable to ambient illumination and multi-device interference.</li><li><strong>Passive stereo</strong>&nbsp;struggles in texture-less regions, where expensive global optimization techniques are required — especially in traditional non-learning based methods.</li></ul>



<p id="03e8">An additional depth sensor type offers a potential solution. In&nbsp;<strong>active stereo</strong>, an infrared stereo camera pair is used, with a pseudorandom pattern projectively texturing the scene via a patterned IR light source (Fig. 3). With a proper selection of wavelength sensing, the camera pair captures a combination of active illumination and passive light, which improves on the quality of structured light while providing a robust solution in both indoor and outdoor scenarios.</p>



<p><a href="https://heartbeat.fritz.ai/activestereonet-the-first-deep-learning-solution-for-active-stereo-systems-f52ed2c6cd2">The full story is available Medium</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2020/12/28/travel-tips/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Why you should Double-DIP for Natural Image Decomposition</title>
		<link>./../../../2020/12/28/staying-organized/index.html</link>
					<comments>./../../../2020/12/28/staying-organized/index.html#respond</comments>
		
		<dc:creator><![CDATA[root]]></dc:creator>
		<pubDate>Mon, 28 Dec 2020 17:49:24 +0000</pubDate>
				<category><![CDATA[Blogs]]></category>
		<guid isPermaLink="false">https://erezposnerportfolio.wordpress.com/?p=6</guid>

					<description><![CDATA[Unsupervised Image Decomposition via Coupled Deep-Image-Priors Many computer vision tasks aspire to decompose an image into its sole components. In Image segmentation, the image is decomposed into meaningful sub-regions, e.g. foreground and background. In transparency separation, the image is separated into its superimposed reflection and transmission. Another example is the task of image dehazing where&#8230;&#160;<a href="./../../../2020/12/28/staying-organized/index.html" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">Why you should Double-DIP for Natural Image Decomposition</span></a>]]></description>
										<content:encoded><![CDATA[
<p><strong>Unsupervised Image Decomposition via Coupled Deep-Image-Priors</strong></p>



<p id="ba58">Many computer vision tasks aspire to decompose an image into its sole components. In Image segmentation, the image is decomposed into meaningful sub-regions, e.g. foreground and background. In transparency separation, the image is separated into its superimposed reflection and transmission. Another example is the task of image dehazing where the goal is to separate a foggy image into its underlying haze-free image and the obscuring fog layers.</p>



<p id="a24e">While appear unrelated at first,&nbsp;these tasks can be viewed as a special case of image decomposition into separate layers. For example, as visualized in Figure 1; image segmentation (separation into foreground and background layers); transparent layer separation (into reflection and transmission layers); Image dehazing (separation into a clear image and a haze map), and more.</p>



<p id="1cdf">In this post, we are going to focus on “Double-DIP”, a unified framework for unsupervised layer decomposition of a single image, based on several&nbsp;<a href="https://towardsdatascience.com/how-to-perform-image-restoration-absolutely-dataset-free-d08da1a1e96d">“Deep-image-Prior” (DIP)&nbsp;</a>networks.</p>



<p><a href="https://towardsdatascience.com/why-you-should-double-dip-for-natural-image-decomposition-ee65b1c1c9bc">The full story is available Medium</a></p>
]]></content:encoded>
					
					<wfw:commentRss>./../../../2020/12/28/staying-organized/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
	</channel>
</rss>
